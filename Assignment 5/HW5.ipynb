{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Flatten, add, Embedding, SimpleRNN, GRU, LSTM, Masking\n",
    "from keras.layers import Conv2D, Dense, Activation, Concatenate\n",
    "from keras.layers.core import Activation, Reshape\n",
    "from keras.layers import Dropout, BatchNormalization, MaxPooling2D, Conv2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = np.genfromtxt('train.tsv',delimiter ='\\t', dtype = \"str\")\n",
    "val_data_raw = np.genfromtxt('val.tsv',delimiter ='\\t', dtype = \"str\")\n",
    "test_data_raw = np.genfromtxt('test.tsv',delimiter ='\\t', dtype = \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length = 161\n",
      "train data samples = (80175, 162)\n",
      "test data samples = (14960, 162)\n",
      "val data samples = (11759, 162)\n",
      "\n",
      "['en' '<S>' 'I' ' ' 'h' 'a' 'v' 'e' ' ' 'g' 'a' 'i' 'n' 'e' 'd' ' ' 'l'\n",
      " 'e' 'v' 'e' 'l' ' ' '3' '9' ' ' 'i' 'n' ' ' 'T' 'h' 'e' ' ' 'T' 'r' 'i'\n",
      " 'b' 'e' 'z' ' ' 'a' 'n' 'd' ' ' 'C' 'a' 's' 't' 'l' 'e' 'z' '!' ' ' 'C'\n",
      " 'a' 'n' ' ' 'y' 'o' 'u' ' ' 'a' 'c' 'h' 'i' 'e' 'v' 'e' ' ' 'i' 't' ' '\n",
      " 'a' 's' ' ' 'w' 'e' 'l' 'l' '?' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>'\n",
      " '</S>' '</S>' '</S>' '</S>' '</S>' '</S>' '</S>']\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(train_data_raw)):\n",
    "    train_data.append([train_data_raw[i][0], ['<S>'] + list(train_data_raw[:,1][i]) + ['</S>']])\n",
    "for i in range(len(val_data_raw)):\n",
    "    val_data.append([val_data_raw[i][0], ['<S>'] + list(val_data_raw[:,1][i]) + ['</S>']])\n",
    "for i in range(len(test_data_raw)):\n",
    "    test_data.append([test_data_raw[i][0], ['<S>'] + list(test_data_raw[:,1][i]) + ['</S>']])\n",
    "\n",
    "# find the vector len by calculating max length\n",
    "vec_len = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][1]) > vec_len:\n",
    "        vec_len = len(train_data[i][1])\n",
    "for i in range(len(val_data)):\n",
    "    if len(val_data[i][1]) > vec_len:\n",
    "        vec_len = len(val_data[i][1])\n",
    "for i in range(len(test_data)):\n",
    "    if len(test_data[i][1]) > vec_len:\n",
    "        vec_len = len(test_data[i][1])\n",
    "print(\"Vector length =\", vec_len)\n",
    "\n",
    "# padding with </S>\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][1]) < vec_len:\n",
    "        pads = ['</S>' for c in range(vec_len - len(train_data[i][1]))]\n",
    "        train_data[i][1] += pads\n",
    "for i in range(len(test_data)):\n",
    "    if len(test_data[i][1]) < vec_len:\n",
    "        pads = ['</S>' for c in range(vec_len - len(test_data[i][1]))]\n",
    "        test_data[i][1] += pads\n",
    "for i in range(len(val_data)):\n",
    "    if len(val_data[i][1]) < vec_len:\n",
    "        pads = ['</S>' for c in range(vec_len - len(val_data[i][1]))]\n",
    "        val_data[i][1] += pads\n",
    "\n",
    "train_data_np = np.zeros((len(train_data), vec_len+1), dtype=object)\n",
    "val_data_np = np.zeros((len(val_data), vec_len+1), dtype=object)\n",
    "test_data_np = np.zeros((len(test_data), vec_len+1), dtype=object)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data_np[i] = np.append(np.array(train_data[i][0]), np.array(train_data[i][1]))\n",
    "for i in range(len(val_data)):\n",
    "    val_data_np[i] = np.append(np.array(val_data[i][0]), np.array(val_data[i][1]))\n",
    "for i in range(len(test_data)):\n",
    "    test_data_np[i] = np.append(np.array(test_data[i][0]), np.array(test_data[i][1]))\n",
    "\n",
    "print('train data samples =', train_data_np.shape)\n",
    "print('test data samples =', test_data_np.shape)\n",
    "print('val data samples =', val_data_np.shape)\n",
    "print()\n",
    "print(train_data_np[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab list initialized with start, stop and Out-Of-Vocab tokens respectively\n",
    "vocab = {'<S>':0, '</S>':0, 'OOV':0}\n",
    "total_char_count = 0\n",
    "\n",
    "# for each sample\n",
    "for i in range(len(train_data_np)):\n",
    "    # for each char in tweet\n",
    "    for c in list(train_data_np[i,1:]):\n",
    "        if c in vocab:\n",
    "            vocab[c] += 1\n",
    "        else:\n",
    "            total_char_count += 1\n",
    "            vocab[c] = 1\n",
    "\n",
    "# calculate OOV tokens\n",
    "oov_chars = []\n",
    "for c in vocab:\n",
    "    if c != 'OOV' and vocab[c] < 10:\n",
    "        vocab['OOV'] += 1\n",
    "        oov_chars.append(c)\n",
    "\n",
    "# remove OOV tokens\n",
    "for c in oov_chars:\n",
    "    vocab.pop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  4, ...,  1,  1,  1],\n",
       "       [ 0, 30, 10, ...,  1,  1,  1],\n",
       "       [ 0, 42,  9, ...,  1,  1,  1],\n",
       "       ...,\n",
       "       [ 0, 49, 18, ...,  1,  1,  1],\n",
       "       [ 0, 66, 22, ...,  1,  1,  1],\n",
       "       [ 0, 49, 26, ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulory encodings\n",
    "vocab_encoding = {}\n",
    "count = 0\n",
    "for c in vocab:\n",
    "  vocab_encoding[c] = count\n",
    "  count += 1\n",
    "\n",
    "encode_char = np.vectorize(lambda c : vocab_encoding[c] if c in vocab else vocab_encoding['OOV'])\n",
    "train_data_np[:,1:] = encode_char(train_data_np[:,1:])\n",
    "val_data_np[:,1:] = encode_char(val_data_np[:,1:])\n",
    "test_data_np[:,1:] = encode_char(test_data_np[:,1:])\n",
    "\n",
    "np.array(train_data_np[:,1:], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pt' 0 42 9 8 7 4 6 5 8 15 5 10 43 10 12 5 44 15 7 16 10 6 7 12 10 7 9 8\n",
      " 18 7 10 15 5 8 31 28 10 15 5 12 15 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['es' 0 34 5 6 5 13 7 13 9 28 12 10 162 10 162 10 162 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['en' 0 53 28 22 13 10 9 6 7 14 9 8 7 15 9 28 8 10 28 57 10 55 20 92 10 40\n",
      " 9 11 10 31 5 13 10 9 12 10 7 4 40 7 39 12 10 35 5 15 15 5 13 10 15 31 7 8\n",
      " 10 15 31 5 10 13 5 7 4 9 15 39 16 10 17 28 12 10 40 31 5 8 10 12 31 5 10\n",
      " 17 28 6 5 12 10 9 8 15 28 10 18 10 31 28 22 12 5 10 7 8 18 10 13 5 6 28\n",
      " 11 5 12 10 15 31 5 10 63 7 18 5 18 10 35 13 7 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_np[2,:])\n",
    "print(val_data_np[2,:])\n",
    "print(test_data_np[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['ca' 'de' 'en' 'es' 'eu' 'fr' 'gl' 'it' 'pt']\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(train_data_np[:,1:], dtype=int)\n",
    "y_train = np.array(train_data_np[:,0])\n",
    "\n",
    "x_test = np.array(test_data_np[:,1:], dtype=int)\n",
    "y_test = np.array(test_data_np[:,0])\n",
    "\n",
    "x_val = np.array(val_data_np[:,1:], dtype=int)\n",
    "y_val = np.array(val_data_np[:,0])\n",
    "\n",
    "\n",
    "classes = np.unique(y_val)\n",
    "print(len(classes))\n",
    "print(classes)\n",
    "\n",
    "\n",
    "i=-1\n",
    "for c in classes:\n",
    "    i=i+1\n",
    "    y_train = np.where(y_train == c,i,y_train)\n",
    "    y_test = np.where(y_test == c,i,y_test)\n",
    "    y_val = np.where(y_val == c,i,y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_data.npz', X = x_train, y = y_train)\n",
    "np.savez('val_data.npz', X = x_val, y = y_val)\n",
    "np.savez('test_data.npz', X = x_test, y = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Warm-up: Perplexity of a Unigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Create vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your vocabulary to be the set of characters that occur at least 10 times in the training data.\n",
    "#Everything else should be mapped to a special token for out-of-vocabulary tokens that should also be included \n",
    "#There are two additional special tokens that should be included in the vocabulary: \n",
    "#the start token <S> and the end token </S>. These are added to the beginning and the end of each Tweet.\n",
    "\n",
    "vocab = {}\n",
    "vocab_val = {}\n",
    "vocab_test = {}\n",
    "max_len = 0\n",
    "max_len_val = 0\n",
    "max_len_test = 0\n",
    "\n",
    "for i in range(0, len(train)):\n",
    "    \n",
    "    tweet_train = train[i][1]\n",
    "    chars_train = list(tweet_train)\n",
    "    max_len = max(max_len, len(chars_train))\n",
    "        \n",
    "    for ch in chars_train:\n",
    "        \n",
    "        if ch in vocab.keys():\n",
    "            vocab[ch] = vocab[ch] + 1\n",
    "        else:\n",
    "            vocab[ch] = 1\n",
    "\n",
    "for i in range(0, len(val)):\n",
    "    \n",
    "    tweet_val = val[i][1]\n",
    "    chars_val = list(tweet_val)\n",
    "    max_len_val = max(max_len_val, len(chars_val))        \n",
    "    for ch in chars_val:\n",
    "        \n",
    "        if ch in vocab_val.keys():\n",
    "            vocab_val[ch] = vocab_val[ch] + 1\n",
    "        else:\n",
    "            vocab_val[ch] = 1\n",
    "            \n",
    "for i in range(0, len(test)):\n",
    "    \n",
    "    tweet_test = test[i][1]\n",
    "    chars_test = list(tweet_test)\n",
    "    max_len_test = max(max_len_test, len(chars_test))\n",
    "        \n",
    "    for ch in chars_test:\n",
    "        \n",
    "        if ch in vocab_test.keys():\n",
    "            vocab_test[ch] = vocab_test[ch] + 1\n",
    "        else:\n",
    "            vocab_test[ch] = 1           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "159\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(max_len)\n",
    "print(max_len_val)\n",
    "print(max_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<S>'] = len(train)\n",
    "vocab['</S>'] = len(train)\n",
    "vocab['Oov']  = 0\n",
    "vocab_ = {}\n",
    "\n",
    "for k,v in vocab.items():\n",
    "    \n",
    "    if (v < 10):\n",
    "        vocab['Oov'] = vocab['Oov'] + 1\n",
    "    else:\n",
    "        vocab_[k] = v\n",
    "        \n",
    "vocab_['Oov'] = vocab['Oov']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dictionary:  494\n",
      "Percent of out of vocabulary words: 0.016351%\n"
     ]
    }
   ],
   "source": [
    "print('Size of the dictionary: ', len(vocab_))\n",
    "print('Percent of out of vocabulary words: %f%%' % (vocab_['Oov']/sum(vocab_.values())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Entropy and Perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute relative frequency\n",
    "\n",
    "total_nw = sum(vocab_.values()) - vocab_['<S>']\n",
    "relative_freq_train = {}\n",
    "\n",
    "\n",
    "for k,v in vocab_.items():\n",
    "    \n",
    "        if (v == 0):\n",
    "            relative_freq_train[k] = 1\n",
    "        else:\n",
    "            relative_freq_train[k] = v/total_nw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_val = deepcopy(vocab_)\n",
    "vocab_val = dict.fromkeys(vocab_val, 0)\n",
    "\n",
    "for i in range(0,len(val)):\n",
    "    \n",
    "    char_list = list(val[i][1])\n",
    "    \n",
    "    for j in char_list:\n",
    "        \n",
    "        if j in vocab_val.keys():\n",
    "            vocab_val[j] = vocab_val[j] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_val['<S>'] = len(val)\n",
    "vocab_val['</S>'] = len(val)\n",
    "\n",
    "total_nw_val = sum(vocab_val.values()) - vocab_val['<S>']\n",
    "relative_freq_val = {}\n",
    "\n",
    "\n",
    "for k,v in vocab_val.items():\n",
    "    \n",
    "    if (k != '<S>'):\n",
    "        \n",
    "        if (v == 0):\n",
    "            relative_freq_val[k] = 1\n",
    "        else:\n",
    "            relative_freq_val[k] = v/total_nw_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy : 5.023521530828288\n",
      "Perplexity : 32.526000522678245\n"
     ]
    }
   ],
   "source": [
    "#Compute entropy\n",
    "entropy = 0\n",
    "\n",
    "for k,v in relative_freq_val.items():\n",
    "    \n",
    "    entropy = entropy + (relative_freq_train[k]*np.log2(relative_freq_val[k]))\n",
    "\n",
    "entropy = -1*entropy\n",
    "perplexity = 2**(entropy)    \n",
    "\n",
    "print('Entropy :', entropy)\n",
    "print('Perplexity :', perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Option B: Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this problem, sequence length refers to the number of characters in a Tweet and batch\n",
    "#size refers to the number of Tweets processed in parallel. Batch sizes greater than one\n",
    "#are used to get better estimates of the gradients.\n",
    "#Prepend all of the sequences with the start of sentence token <S> and append the end\n",
    "#of sentence token </S>. It is convienent to define a maximum sequence length and then\n",
    "#pad all the sequences to have that length. You can use the </S> token for padding.\n",
    "#Keep track of the original (before padding) length of each sequence. You also need to\n",
    "#create a table that can map characters to ids and a separate table that maps languages\n",
    "#to ids.\n",
    "\n",
    "batch_size = 32\n",
    "max_tweet_len = 161\n",
    "og_size = []\n",
    "padded_tweets = []\n",
    "\n",
    "      \n",
    "for i in range(0, len(train)):\n",
    "    \n",
    "    tweet = train[i][1]\n",
    "    og_size.append(len(tweet))\n",
    "    tweet_char = list(tweet)\n",
    "    tweet_char.insert(0, '<S>')\n",
    "    \n",
    "    if len(tweet_char) < max_tweet_len:\n",
    "        tweet_char.extend(repeat('</S>', max_tweet_len - len(tweet_char)))\n",
    "    padded_tweets.append(tweet_char)\n",
    "\n",
    "padded_tweets= np.array(padded_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "#np.savez('padded.npz', tweets = padded_tweets)\n",
    "print(len(padded_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this problem, sequence length refers to the number of characters in a Tweet and batch\n",
    "#size refers to the number of Tweets processed in parallel. Batch sizes greater than one\n",
    "#are used to get better estimates of the gradients.\n",
    "#Prepend all of the sequences with the start of sentence token <S> and append the end\n",
    "#of sentence token </S>. It is convienent to define a maximum sequence length and then\n",
    "#pad all the sequences to have that length. You can use the </S> token for padding.\n",
    "#Keep track of the original (before padding) length of each sequence. You also need to\n",
    "#create a table that can map characters to ids and a separate table that maps languages\n",
    "#to ids.\n",
    "\n",
    "batch_size = 32\n",
    "max_tweet_len = 161\n",
    "og_size = []\n",
    "padded_tweets_val = []\n",
    "\n",
    "      \n",
    "for i in range(0, len(val)):\n",
    "    \n",
    "    tweet = val[i][1]\n",
    "    og_size.append(len(tweet))\n",
    "    tweet_char = list(tweet)\n",
    "    tweet_char.insert(0, '<S>')\n",
    "    \n",
    "    if len(tweet_char) < max_tweet_len:\n",
    "        tweet_char.extend(repeat('</S>', max_tweet_len - len(tweet_char)))\n",
    "    padded_tweets_val.append(tweet_char)\n",
    "\n",
    "padded_tweets_val= np.array(padded_tweets_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_tweet_len = 161\n",
    "og_size = []\n",
    "padded_tweets_test = []\n",
    "\n",
    "      \n",
    "for i in range(0, len(test)):\n",
    "    \n",
    "    tweet = test[i][1]\n",
    "    og_size.append(len(tweet))\n",
    "    tweet_char = list(tweet)\n",
    "    tweet_char.insert(0, '<S>')\n",
    "    \n",
    "    if len(tweet_char) < max_tweet_len:\n",
    "        tweet_char.extend(repeat('</S>', max_tweet_len - len(tweet_char)))\n",
    "    padded_tweets_test.append(tweet_char)\n",
    "\n",
    "padded_tweets_test= np.array(padded_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14960, 161)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padded = np.load('padded.npz')\n",
    "# padded_tweets = padded['tweets']\n",
    "\n",
    "padded_tweets_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_idx = {}\n",
    "lang_idx = {'en': 0, 'es': 1, 'pt':2, 'gl' :3, 'eu': 4, \n",
    "         'ca' :5, 'fr' : 6, 'it': 7, 'de':8}\n",
    "\n",
    "for idx,k in enumerate(vocab_):\n",
    "    char_idx[k] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-51daacc8a50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mchar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(len(char_list))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'padded_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "ln_idx = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(train)):\n",
    "    \n",
    "    char_list = padded_tweets[i]\n",
    "    #print(len(char_list))\n",
    "    ln = train[i][0]\n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    seq3 = []\n",
    "    \n",
    "    for i in range(0, len(char_list)):\n",
    "        \n",
    "        if char_list[i] not in char_idx.keys():\n",
    "            \n",
    "            char_id = char_idx['Oov']\n",
    "        else:\n",
    "            char_id = char_idx[char_list[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "        dat1 = char_id        \n",
    "        seq1.append(dat1)\n",
    "    seq3.append(lang_idx[ln])\n",
    "\n",
    "    \n",
    "    X.append(seq1)\n",
    "    ln_idx.append(seq3)\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80175, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X); y = np.array(y); ln_idx = np.array(ln_idx)\n",
    "#X = np.reshape(X, (len(X),))\n",
    "ln_idx.shape\n",
    "# for i in range(0, len(X)):\n",
    "    \n",
    "#     if len(X[i]) != max_tweet_len:\n",
    "#         print(i, len(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ln_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-09380efe02e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mln_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ln_idx' is not defined"
     ]
    }
   ],
   "source": [
    "print(ln_idx.shape)\n",
    "np.savez('train_data.npz', X = X,  y= ln_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('train_data.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "ln_idx_val = []\n",
    "\n",
    "\n",
    "for i in range(0, len(val)):\n",
    "    \n",
    "    char_list = padded_tweets_val[i]\n",
    "    #print(len(char_list))\n",
    "    ln = train[i][0]\n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    seq3 = []\n",
    "    \n",
    "    for i in range(0, len(char_list)):\n",
    "        \n",
    "        if char_list[i] not in char_idx.keys():\n",
    "            \n",
    "            char_id = char_idx['Oov']\n",
    "        else:\n",
    "            char_id = char_idx[char_list[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "        dat1 = char_id        \n",
    "        seq1.append(dat1)\n",
    "    seq3.append(lang_idx[ln])\n",
    "\n",
    "    \n",
    "    X_val.append(seq1)\n",
    "    ln_idx_val.append(seq3)\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11759, 1)\n",
      "(11759, 161)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.array(X_val); ln_idx_val = np.array(ln_idx_val)\n",
    "print(ln_idx_val.shape)\n",
    "print(X_val.shape)\n",
    "np.savez('val_data.npz', X = X_val,  y= ln_idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viva la operación bikini ✌️ @coquetaLgancia'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[5]\n",
    "(val[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14960, 161)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_len = 161\n",
    "og_size = []\n",
    "padded_tweets_test = []\n",
    "\n",
    "      \n",
    "for i in range(0, len(test)):\n",
    "    \n",
    "    tweet = test[i][1]\n",
    "    og_size.append(len(tweet))\n",
    "    tweet_char = list(tweet)\n",
    "    tweet_char.insert(0, '<S>')\n",
    "    \n",
    "    if len(tweet_char) < max_tweet_len:\n",
    "        tweet_char.extend(repeat('</S>', max_tweet_len - len(tweet_char)))\n",
    "    padded_tweets_test.append(tweet_char)\n",
    "\n",
    "padded_tweets_test= np.array(padded_tweets_test)\n",
    "padded_tweets_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "ln_idx_test = []\n",
    "\n",
    "\n",
    "for i in range(0, len(test)):\n",
    "    \n",
    "    char_list = padded_tweets_test[i]\n",
    "    #print(len(char_list))\n",
    "    ln = train[i][0]\n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    seq3 = []\n",
    "    \n",
    "    for i in range(0, len(char_list)):\n",
    "        \n",
    "        if char_list[i] not in char_idx.keys():\n",
    "            \n",
    "            char_id = char_idx['Oov']\n",
    "        else:\n",
    "            char_id = char_idx[char_list[i]]\n",
    "            \n",
    "            \n",
    "        \n",
    "        dat1 = char_id        \n",
    "        seq1.append(dat1)\n",
    "    seq3.append(lang_idx[ln])\n",
    "\n",
    "    \n",
    "    X_test.append(seq1)\n",
    "    ln_idx_test.append(seq3)\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14960, 1)\n",
      "(14960, 161)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(X_test); ln_idx_test = np.array(ln_idx_test)\n",
    "print(ln_idx_test.shape)\n",
    "print(X_test.shape)\n",
    "np.savez('test_data.npz', X = X_test,  y= ln_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3d458e44aa3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vocab (InputLayer)           (None, 161)               0         \n",
      "_________________________________________________________________\n",
      "char_embedding (Embedding)   (None, 161, 10)           4940      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 161, 10, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 158, 7, 64)        1088      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 79, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 79, 3, 64)         256       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 15168)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 136521    \n",
      "=================================================================\n",
      "Total params: 142,805\n",
      "Trainable params: 142,677\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "len_vocab = len(vocab_)\n",
    "len_training = len(train)\n",
    "num_lang = 9\n",
    "\n",
    "\n",
    "Vocab = Input(name = 'vocab', shape = [max_tweet_len])\n",
    "\n",
    "char_embedding =  Embedding(name = 'char_embedding', input_dim = len_vocab, output_dim = 10)(Vocab)\n",
    "\n",
    "reshaped_out = Reshape((161,10, 1))(char_embedding)\n",
    "#print(reshaped_out.shape)\n",
    "\n",
    "l1 = Conv2D(64, 4, activation='relu')(reshaped_out)\n",
    "l2 = MaxPooling2D(pool_size=2)(l1)\n",
    "#l3 = Dropout(0.25)(l2)\n",
    "l3 = BatchNormalization()(l2)\n",
    "l4 = Flatten()(l3)\n",
    "#l5 = Dense(128, activation='relu')(l4)\n",
    "out = Dense(num_lang, activation='softmax')(l4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs = Vocab, outputs=out)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80175 samples, validate on 11759 samples\n",
      "Epoch 1/100\n",
      "80175/80175 [==============================] - 208s 3ms/step - loss: 0.7313 - acc: 0.7513 - val_loss: 5.0787 - val_acc: 0.2636\n",
      "Epoch 2/100\n",
      "80175/80175 [==============================] - 186s 2ms/step - loss: 0.4540 - acc: 0.8452 - val_loss: 6.0265 - val_acc: 0.2317\n",
      "Epoch 3/100\n",
      "51072/80175 [==================>...........] - ETA: 1:10 - loss: 0.3690 - acc: 0.8738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d8cd25ca6171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#in_ = {'vocab': X, 'lang': ln_idx}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln_idx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#in_ = {'vocab': X, 'lang': ln_idx}\n",
    "history = model.fit(X, ln_idx, epochs = 100, batch_size=128, verbose=1, validation_data = (X_val, ln_idx_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
